{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea551ebf",
   "metadata": {},
   "source": [
    "Unlocking the Power of RAG Application! ðŸš€\n",
    "\n",
    "Dive into the RAG framework, where identifying the perfect documents is key!\n",
    "Query Magic: âœ¨\n",
    "\n",
    "Your query embarks on a journey through retrievers, armed with cool similarity matrices like cosine, MRR, and Jaccard.\n",
    "The result? A stellar lineup of the top N documents ready for action!\n",
    "Elevate the RAG Pipeline: ðŸŒŸ\n",
    "\n",
    "Supercharge your strategy by blending different indexes â€“ keyword, vector, graph â€“ creating a powerhouse of results.\n",
    "Each database gets its moment in the spotlight, retrieving the dazzling top 10 documents.\n",
    "Rerank for Brilliance: ðŸ”„\n",
    "\n",
    "Unleash the power of reranking! By using multiple indexes, we fine-tune our results to select the crÃ¨me de la crÃ¨me, the top 10 (or let's make it 7! ðŸŽ‰).\n",
    "LLamam Magic Touch: ðŸŒˆ\n",
    "\n",
    "LLamam index swoops in with fantastic strategies, like the Node post processor for epic reranking.\n",
    "Hold onto your hat as we demonstrate reranking using the one and only GPT-3 â€“ a paper-reranker extraordinaire! It's like magic for finding the most relevant documents.\n",
    "Answer Synthesis Extravaganza: ðŸŽŠ\n",
    "\n",
    "GPT-3 joins the party, bringing an exciting and friendly tone to answer synthesis.\n",
    "The result? An answer synthesis with the perfect blend of friendliness and flair! ðŸŒŸ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45aa107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5359da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import llama_index\n",
    "llama_index.set_global_handler(\"simple\")\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "\n",
    "from llama_index import (\n",
    "    KnowledgeGraphIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    "    SimpleKeywordTableIndex\n",
    ")\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.llms.palm import PaLM\n",
    "from llama_index.embeddings import GooglePaLMEmbedding\n",
    "\n",
    "\n",
    "from llama_index.callbacks import (\n",
    "    CallbackManager,\n",
    "    LlamaDebugHandler\n",
    ")\n",
    "\n",
    "\n",
    "from llama_index.retrievers import (\n",
    "    KeywordTableSimpleRetriever\n",
    ")\n",
    "\n",
    "from llama_index import Document, SummaryIndex\n",
    "from llama_index.query_engine import PandasQueryEngine, RetrieverQueryEngine\n",
    "from llama_index.retrievers import RecursiveRetriever\n",
    "from llama_index.schema import IndexNode\n",
    "from llama_hub.file.pymu_pdf.base import PyMuPDFReader\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from llama_index.readers import WikipediaReader\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    SQLDatabase,\n",
    ")\n",
    "\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index.schema import IndexNode\n",
    "from llama_index.response.notebook_utils import display_source_node\n",
    "\n",
    "\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index.schema import IndexNode\n",
    "from llama_index.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17471992",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33451754",
   "metadata": {},
   "outputs": [],
   "source": [
    "palm_api_key  = \"AIzaSyApBCzqW_RF4qbkX9kMoNwjooIqrm8oZEQ\"\n",
    "llm = PaLM(api_key=palm_api_key)\n",
    "\n",
    "model_name = \"models/embedding-gecko-001\"\n",
    "embed_model = GooglePaLMEmbedding(model_name=model_name, api_key=palm_api_key)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "                                    llm = llm,\n",
    "                                    embed_model = embed_model,\n",
    "                                    chunk_size=512,\n",
    "                                    callback_manager=callback_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f44e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor import LLMRerank\n",
    "from llama_index.llms import OpenAI\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4801ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_TOKEN = \"sk-\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428f9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "wiki_titles = [\n",
    "    \"Vincent van Gogh\",\n",
    "]\n",
    "\n",
    "\n",
    "data_path = Path(\"data_wiki\")\n",
    "\n",
    "for title in wiki_titles:\n",
    "    response = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"explaintext\": True,\n",
    "        },\n",
    "    ).json()\n",
    "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "    wiki_text = page[\"extract\"]\n",
    "\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec7bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./data_wiki/\").load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cd5ca50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_node_parsing ->  0.13596 seconds\n",
      "      |_chunking ->  0.111031 seconds\n",
      "    |_embedding ->  3.256311 seconds\n",
      "    |_embedding ->  1.059069 seconds\n",
      "    |_embedding ->  1.058989 seconds\n",
      "    |_embedding ->  1.238246 seconds\n",
      "    |_embedding ->  0.665584 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e35b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.schema import QueryBundle\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# from llama_index.postprocessor import RankGPTRerank\n",
    "\n",
    "\n",
    "def get_retrieved_nodes(\n",
    "    query_str, vector_top_k=10, reranker_top_n=3, with_reranker=False\n",
    "):\n",
    "    query_bundle = QueryBundle(query_str)\n",
    "    # configure retriever\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=index,\n",
    "        similarity_top_k=vector_top_k,\n",
    "    )\n",
    "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "    if with_reranker:\n",
    "        # configure reranker\n",
    "        reranker = RankGPTRerank(\n",
    "            llm=OpenAI(\n",
    "                model=\"gpt-3.5-turbo-16k\",\n",
    "                temperature=0.0,\n",
    "                api_key=OPENAI_API_TOKEN,\n",
    "            ),\n",
    "            top_n=reranker_top_n,\n",
    "            verbose=True,\n",
    "        )\n",
    "        retrieved_nodes = reranker.postprocess_nodes(\n",
    "            retrieved_nodes, query_bundle\n",
    "        )\n",
    "\n",
    "    return retrieved_nodes\n",
    "\n",
    "\n",
    "def pretty_print(df):\n",
    "    return display(HTML(df.to_html().replace(\"\\\\n\", \"<br>\")))\n",
    "\n",
    "\n",
    "def visualize_retrieved_nodes(nodes) -> None:\n",
    "    result_dicts = []\n",
    "    for node in nodes:\n",
    "        result_dict = {\"Score\": node.score, \"Text\": node.node.get_text()}\n",
    "        result_dicts.append(result_dict)\n",
    "\n",
    "    pretty_print(pd.DataFrame(result_dicts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
